# Multimodal-Emotion-Classification
Emotion classification through the integration of multimodal data (text, audio, and images).

# Multimodal Emotion Classification

Emotion analysis through the integration of **text**, **audio**, and **image** data using deep learning models.

---

## ðŸ“‚ Repository Contents

- `app.py` â€“ Main Streamlit application for emotion detection.  
- `requirements.txt` â€“ Required dependencies.  
- `.gitignore` â€“ Prevents large files such as models/scalers from being uploaded to GitHub.  
- `models/` (excluded from repo) â€“ Contains the trained model and scaler `.pkl` files.  
- `data/` (excluded from repo) â€“ Contains lexicon files such as `NRC-VAD-Lexicon.txt`.  

---

## ðŸš€ Run the Application Locally

1. **Clone the repository**  
   ```bash
   git clone https://github.com/FisanSyafa/Multimodal-Emotion-Classification.git
   cd Multimodal-Emotion-Classification
Create a virtual environment (optional but recommended)

2. bash
3. 
\python -m venv venv

Activate the environment:


Windows (PowerShell):


3. bash

venv\Scripts\activate

Linux / macOS:


5. bash

source venv/bin/activate

Install dependencies


7. bash

pip install -r requirements.txt

Download model and scaler files

â€“ Download the models/ folder from this link.
â€“ Place the folder at the root of the project so the structure looks like:

9. Structure
   
â”œâ”€â”€ app.py

â”œâ”€â”€ models/

â”‚   â”œâ”€â”€ best_model_fold_5_savedmodel/

â”‚   â”œâ”€â”€ scaler_vad.pkl

â”‚   â”œâ”€â”€ scaler_bert.pkl

â”‚   â”œâ”€â”€ scaler_audio.pkl

â”‚   â””â”€â”€ scaler_image.pkl

â”œâ”€â”€ data/

â”‚   â””â”€â”€ NRC-VAD-Lexicon.txt

â”œâ”€â”€ requirements.txt

â””â”€â”€ README.md

Run the application


8. bash

streamlit run app.py
